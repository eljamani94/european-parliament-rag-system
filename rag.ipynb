{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üìö Introduction to Retrieval Augmented Generation with LangChain ü¶úüîó\n",
    "\n",
    "In this notebook you'll learn how to use LangChain for Retrieval Augmented Generation.\n",
    "\n",
    "We will use an LLM to answer questions about our own documents!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚öôÔ∏è Setup\n",
    "\n",
    "üëâ Run the cell below to import a couple of basic libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "from pprint import pprint\n",
    "from IPython.display import Markdown"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üëâ Run the cell below to load our API key again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()  # Load environment variables from .env file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìö Why RAG?\n",
    "\n",
    "An LLM on its own can respond questions about everything it has learned.\n",
    "\n",
    "That has a couple of drawbacks:\n",
    "- The training data comes from the past and is not updated with the most recent data.\n",
    "- It only knows the data it was trained on.\n",
    "\n",
    "We want to use an LLM to work with our own data. That is where RAG, or Retrieval-Augmented Generation steps in.\n",
    "\n",
    "1. **Retrieval-Augmented Generation (RAG)** combines a language model with a document retriever to enhance factual accuracy.\n",
    "2. **It retrieves relevant external documents** (e.g., from a knowledge base) before generating responses.\n",
    "3. **The language model uses both the prompt and retrieved context** to produce more informed and grounded outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üá™üá∫ Context\n",
    "\n",
    "In this challenge, we'll work with documents from the European Parliament.\n",
    "\n",
    "Imagine you're a reporter, and you want to know what has been said about a certain topic during the European Parliament's plenary sessions. Those sessions take place 12 times a year in Strassbourg, and last 4 days. Transcriptions of the sessions are available on the EP's website.\n",
    "\n",
    "You definitely don't want to go ploughing through all those transcripts. So, let's leverage RAG to make our life easier!\n",
    "\n",
    "This is good data to work with, because at all times we can take brand new data to test it out."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìò Let's get the data\n",
    "\n",
    "1. Head to the [EP's website](https://www.europarl.europa.eu/plenary/en/debates-video.html). \n",
    "1. That will lead you to the most recent plenary session.\n",
    "1. Under the first date, click on `HTML` in \"‚ñ∂Ô∏è Verbatim reports HTML\".\n",
    "1. Scroll to the bottom of the page, and download the PDF file at the bottom.\n",
    "1. Save the file in the `data` folder.\n",
    "\n",
    "We'll start with one document, but you can already download the same for a couple of other days for later.\n",
    "\n",
    "Have a look at the document. How many pages does it have? Quickly scroll through the document to get a feel for it.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üî¢ Embedding documents\n",
    "\n",
    "Embedding documents means that we will translate whole documents, or chunks of documents, into vectors.\n",
    "\n",
    "LangChainü¶úüîó will be very helpful again.\n",
    "\n",
    "Let's instantiate an embedder and try it out. Because we're using Gemini as our LLM, let's stick to Google's text embedders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "\n",
    "embeddings = GoogleGenerativeAIEmbeddings(model=\"models/text-embedding-004\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üëâ Try the embedder's `.embed_query()` to embed a simple piece of text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "steps"
    ]
   },
   "outputs": [],
   "source": [
    "# Embed a text like \"What is the capital of France?\" and save it to a variable `sample_embedding`\n",
    "\n",
    "# YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üëâ Take the time to explore this `sample_embedding`. What does it look like? What's its type? What is the embedding size?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üíæ Load our real data from PDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we know what an embedding looks like, it's time to get working with our real data.\n",
    "\n",
    "üëâ Head to the [LangChain documentation](https://python.langchain.com/docs/how_to/document_loader_pdf/), and find out how you can load a PDF.\n",
    "\n",
    "In the documentation you'll see an `async for` loop. Let's not get into *[asynchronous programming](https://en.wikipedia.org/wiki/Asynchrony_(computer_programming))* now, it makes things too complex. Use this instead:\n",
    "\n",
    "```python\n",
    "for page in loader.lazy_load():\n",
    "```\n",
    "\n",
    "üëâ Then go ahead and load one of the PDFs you downloaded before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üëâ Explore the `pages`:\n",
    "- What is its data type?\n",
    "- How many pages do you have?\n",
    "- What is the type of one page?\n",
    "- How can you access the content of one page?\n",
    "- How many characters does the full document have?\n",
    "- What is in the `metadata` of a page?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚úÇÔ∏è Split our data\n",
    "\n",
    "Our complete document is too long to be embedded. Our text embedder can take inputs up to 2.048 tokens. For Gemini models that is about 8.196 characters (4 characters per token).\n",
    "\n",
    "So we want to split our document in smaller chunks.\n",
    "\n",
    "We already have a bunch of pages we could work with. But page ends are a bit arbitrary: they usually appear in the middle of a sentence.\n",
    "\n",
    "Also, there is no overlap between the pages. So the first line of a page misses all context before. It's better to split the full text with a bit of overlap."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we'll load the PDF again, this time without splitting it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = PyPDFLoader(file_path, mode='single')\n",
    "pdf_text = loader.load()\n",
    "len(pdf_text[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our whole PDF as one document, we can split it in chunks in a smarter way.\n",
    "\n",
    "üëâ Again, head over to the [LangChain documentation](https://python.langchain.com/docs/how_to/recursive_text_splitter/) and find out how to split our `pages` into chunks (called `documents` in LangChain).\n",
    "\n",
    "Split it in chunks of 2_000 characters (that's about half a page in our case) with an overlap of 400. You can experiment with other values if you want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üëâ Inspect `all_splits`:\n",
    "- What is its data type?\n",
    "- How many splits do you have?\n",
    "- What is the type of one split?\n",
    "- How can you access the content of one split?\n",
    "- How many characters do we have in total now?\n",
    "- What is in the `metadata` of a split?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üóÑÔ∏è Bring it all together: embed and store our documents in a vector store\n",
    "\n",
    "We have:\n",
    "- An embedder\n",
    "- A loader to load the data\n",
    "- A text splitter to split our document into documents\n",
    "\n",
    "What's missing?\n",
    "\n",
    "We can embed our documents, but we want to store them somewhere. That's where a vector store comes in: it allows us to save:\n",
    "- the document (the chunk),\n",
    "- its embedding,\n",
    "- its metadata.\n",
    "\n",
    "In a next step we'll then be able to retrieve documents efficiently.\n",
    "\n",
    "üëâ Check the [LangChain documentation](https://python.langchain.com/docs/concepts/vectorstores/) to see how you can create an `InMemoryVectorStore`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "steps"
    ]
   },
   "outputs": [],
   "source": [
    "# Import the necessary libraries\n",
    "\n",
    "# YOUR CODE HERE\n",
    "\n",
    "# Create an in-memory vector store using the embedder `embeddings` we created earlier\n",
    "\n",
    "# YOUR CODE HERE\n",
    "\n",
    "# Add the `all_splits` to the vector store and store the result in a variable called `document_ids`\n",
    "\n",
    "# YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "steps"
    ]
   },
   "outputs": [],
   "source": [
    "# Have a look at the first 3 document IDs\n",
    "\n",
    "# YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "steps"
    ]
   },
   "outputs": [],
   "source": [
    "# Use the vector store's `get_by_ids` method. You have to give it a list of document IDs.\n",
    "\n",
    "# YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üëâ How can you access a vector store's document's content and metadata?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîé Use the vector store to retrieve similar documents\n",
    "\n",
    "Now that we embedded the documents, we can use the vector store to retrieve similar documents.\n",
    "\n",
    "üëâ Check in the (LangChain documentation](https://python.langchain.com/docs/concepts/vectorstores/) how that works.\n",
    "\n",
    "Use a query, e.g. \"Summarize the discussion on agricultural policy.\", and find the most similar documents. You can also specify the number of documents to retrieve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "steps"
    ]
   },
   "outputs": [],
   "source": [
    "# Save your question into a variable called `query`\n",
    "\n",
    "# YOUR CODE HERE\n",
    "\n",
    "# Use the vector store to find similar documents to the query. Store the result in a variable called `retrieved_docs`\n",
    "\n",
    "# YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This concludes the so-called \"Retrieval\" part of RAG: we can now find the documents that are the most similar to our query.\n",
    "\n",
    "Most of the work is done now!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üí¨ Generate an answer to our question\n",
    "\n",
    "So far we only used an **embedding model** to enable us to retrieve the most similar documents.\n",
    "\n",
    "Now, we will use a generative LLM to get an answer to our question: we'll feed it with our retrieved documents, and our question.\n",
    "\n",
    "The most rudimentary way to do this would be to concatenate all our inputs together, add our question, and see the result.\n",
    "\n",
    "Let's give it a try.\n",
    "\n",
    "üëâ First instantiate an LLM like in the previous challenges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then create a rudimentary prompt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = '\\n\\n'.join([doc.page_content for doc in retrieved_docs])\n",
    "prompt += \"\\n\\n\" + query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üëâ Now use the prompt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's not bad, but we could do better by writing a more extensive prompt, giving the model more guidance.\n",
    "\n",
    "It turns out we're not the first ones doing this, and LangChain has a library of pre-made prompts for us.\n",
    "\n",
    "üëâ Run the cell below, and try to understand how it works. (You'll get a warning about LangSmithMissingAPIKeyWarning, you can disregard that.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import hub\n",
    "\n",
    "prompt_template = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "example_messages = prompt_template.invoke(\n",
    "    {\"context\": \"(context goes here)\", \"question\": \"(question goes here)\"}\n",
    ").to_messages()\n",
    "\n",
    "print(\"\\n\")\n",
    "print(example_messages[0].content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See how LangChain generated a more precise prompt for us? Let's use this for our RAG!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üëâ First, join all retrieved docs into one long string, separated by two newlines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üëâ Next, create a `prompt` starting from your query and the retrieved documents. Remember to look at the example above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üëâ Finally use the LLM model with `the_prompt` we just created:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üéâ We have finished our first RAG: the LLM generated text ***grounded*** in the documents we provided it with."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üíæ Persisting our embeddings\n",
    "\n",
    "So far we worked with an in-memory vector store. So when you will close your notebook, you will also loose all the embeddings.\n",
    "\n",
    "‚ö†Ô∏è Remember that these embeddings are generated by models running on your provider's platform, in this case on Google's machines. And they don't work for free. üí∞\n",
    "\n",
    "For one, relatively small document like this one, the cost is low, but it quickly adds up. So far, we just workend on one day's transcripts. There are 3 more per session, 12 sessions per year, multiple years...\n",
    "\n",
    "To solve this we will just replace our vector store by a persistent one. That's the advantage of LangChain: it's very easy to replace components.\n",
    "\n",
    "Our in-memory vector store was great for experimenting, now we'll switch it for another one. We will use [Chroma](https://www.trychroma.com/), a very popular vector store. We can run it locally, and use it through LangChain.\n",
    "\n",
    "We'll recreate our whole flow. It's a good exercise to try to bring it all together again in a couple of code cells. At the same time we'll refactor everything into some reusable code.\n",
    "\n",
    "We want to have two functions in the end:\n",
    "\n",
    "1. `embed_and_store()`: Add another session's transcript to our vector db, so that we have more data to retrieve from.\n",
    "2. `answer()`: Query our vector store with different questions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Instantiate a Chroma vector store\n",
    "\n",
    "üëâ Look at [LangChain's documentation](https://python.langchain.com/docs/integrations/vectorstores/chroma/) to see how to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Create `embed_and_store()`\n",
    "\n",
    "üëâ Complete the code for this function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_and_store(file_path, vector_store):\n",
    "    \"\"\"Load a PDF file, split it into chunks, and store the chunks in a vector store.\"\"\"\n",
    "    # Load the PDF file\n",
    "    pass  # YOUR CODE HERE\n",
    "\n",
    "    # Split the pages into chunks\n",
    "    pass  # YOUR CODE HERE\n",
    "\n",
    "    # # Add the session_date to the metadata\n",
    "    # for split in all_splits:\n",
    "    #     split.metadata['session_date'] = session_date\n",
    "\n",
    "    # Add the chunks to the vector store\n",
    "    pass  # YOUR CODE HERE\n",
    "\n",
    "    return document_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üëâ Try out your function with a file or even two:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Create `answer()`\n",
    "\n",
    "üëâ Complete the code for this function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer(query, vector_store, llm, prompt_template=None):\n",
    "    \"\"\"Answer a query using the vector store and the language model.\"\"\"\n",
    "    # Retrieve similar documents from the vector store\n",
    "    retrieved_docs = vector_store.similarity_search(query, k=6)\n",
    "\n",
    "    # Create the prompt\n",
    "    docs_content = \"\\n\\n\".join(doc.page_content for doc in retrieved_docs)\n",
    "\n",
    "    # If no prompt template is provided, use the default one\n",
    "    if not prompt_template:\n",
    "        prompt_template = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "    prompt = prompt_template.invoke(\n",
    "        {\"context\": docs_content, \"question\": query}\n",
    "    )\n",
    "\n",
    "    # Get the answer from the language model\n",
    "    answer = llm.invoke(prompt)\n",
    "\n",
    "    return answer.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üëâ Try out your function with a query of your liking:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üèÅ Congratulations! You now master RAG using LangChain, and you learned how to make reusable functions to add more documents to your vector store, and to query it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Optional] Adding metadata\n",
    "\n",
    "The RAG we set up queries all the documents from the vector store. Imagine we have multiple year's information in there. It would be handy if we could filter on years, or dates, no?\n",
    "\n",
    "How to do that? Remember that the documents in the vector store contain metadata. If we could add the date to it, we could use it later to filter.\n",
    "\n",
    "Tip: Add your metadata as early as possible in your pipeline. Don't try to add it after your data was already stored to the vector store.\n",
    "\n",
    "üëâ Adapt your `embed_and_store()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_and_store_fancy(file_path, vector_store, session_date):\n",
    "    \"\"\"Load a PDF file, split it into chunks, and store the chunks in a vector store.\n",
    "    Session_date is added to the metadata of each chunk.\"\"\"\n",
    "    pass  # YOUR CODE HERE\n",
    "\n",
    "    return document_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üëâ Try out your function and check that your vector store contains the extra metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have to limit the retriever to the date asked by the user. \n",
    "\n",
    "üëâ Adapt your `answer()` function so it can take a date and filter documents based on the new metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice! You have combined similarity search with metadata search to create a powerful RAG system!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
